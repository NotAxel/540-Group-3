{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c5e1ad-5d5f-468a-b996-dc5a98b5e041",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ca22664-cb4d-4586-8384-44e59c381b1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:44:37.695200Z",
     "iopub.status.busy": "2025-10-17T02:44:37.694927Z",
     "iopub.status.idle": "2025-10-17T02:44:37.700673Z",
     "shell.execute_reply": "2025-10-17T02:44:37.699970Z",
     "shell.execute_reply.started": "2025-10-17T02:44:37.695177Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import io\n",
    "from datetime import datetime\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, TransformStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet, Join\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.inputs import TrainingInput, TransformInput\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.model_monitor import (\n",
    "    DataCaptureConfig,\n",
    "    DefaultModelMonitor,\n",
    "    CronExpressionGenerator,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71cdee1c-eb02-41b9-8f99-b90026c4fd8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:44:41.859929Z",
     "iopub.status.busy": "2025-10-17T02:44:41.859659Z",
     "iopub.status.idle": "2025-10-17T02:44:42.544313Z",
     "shell.execute_reply": "2025-10-17T02:44:42.543591Z",
     "shell.execute_reply.started": "2025-10-17T02:44:41.859909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SageMaker Role ARN: arn:aws:iam::115718999037:role/LabRole\n",
      " Region: us-east-1\n",
      " Default S3 Bucket: sagemaker-us-east-1-115718999037\n",
      " Project Bucket: aai540-ecommerce-recommendation-project-group3\n"
     ]
    }
   ],
   "source": [
    "# Initialize AWS services and sessions\n",
    "sagemaker_session = sagemaker.Session()\n",
    "pipeline_session = PipelineSession()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = get_execution_role()\n",
    "s3_client = boto3.client('s3')\n",
    "sm_client = boto3.client('sagemaker')\n",
    "cw_client = boto3.client('cloudwatch')\n",
    "\n",
    "# Configuration\n",
    "BUCKET_NAME = 'aai540-ecommerce-recommendation-project-group3'\n",
    "S3_PREFIX = 'mlops-pipeline'\n",
    "PROCESSED_PREFIX = f'{S3_PREFIX}/processed/'\n",
    "FEATURE_STORE_PREFIX = f'{S3_PREFIX}/feature-store/'\n",
    "MODEL_PACKAGE_GROUP_NAME = 'AAI540ECommerceRecommendationGroup'\n",
    "\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f\"\\n SageMaker Role ARN: {role}\")\n",
    "print(f\" Region: {region}\")\n",
    "print(f\" Default S3 Bucket: {default_bucket}\")\n",
    "print(f\" Project Bucket: {BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c9dffe-7dd6-4d1d-bcb5-7bb0a643b05c",
   "metadata": {},
   "source": [
    "## Section 2: Data Loading and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9085ef7-67a0-498a-9d0f-c92825b7df59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:44:45.863276Z",
     "iopub.status.busy": "2025-10-17T02:44:45.862991Z",
     "iopub.status.idle": "2025-10-17T02:44:45.868222Z",
     "shell.execute_reply": "2025-10-17T02:44:45.867517Z",
     "shell.execute_reply.started": "2025-10-17T02:44:45.863255Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training and validation data from S3\n",
    "def load_data_from_s3(bucket_name=BUCKET_NAME, s3_prefix=''):\n",
    "    \n",
    "    train_key = f\"{s3_prefix}train_user_product_pairs.csv\"\n",
    "    val_key = f\"{s3_prefix}validation_user_product_pairs.csv\"\n",
    "    \n",
    "    print(f\"\\n→ Loading training data from s3://{bucket_name}/{train_key}\")\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=train_key)\n",
    "    df_train = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "    print(f\"   Training data loaded: {df_train.shape}\")\n",
    "    \n",
    "    print(f\"\\n→ Loading validation data from s3://{bucket_name}/{val_key}\")\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=val_key)\n",
    "    df_val = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "    print(f\"   Validation data loaded: {df_val.shape}\")\n",
    "    \n",
    "    # Combine for proper splitting\n",
    "    df_all = pd.concat([df_train, df_val], ignore_index=True)\n",
    "    print(f\"\\n Combined dataset: {df_all.shape}\")\n",
    "    \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91f76eb4-900b-4707-b4dd-fd86c3ec9d2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:44:50.335153Z",
     "iopub.status.busy": "2025-10-17T02:44:50.334833Z",
     "iopub.status.idle": "2025-10-17T02:45:08.258613Z",
     "shell.execute_reply": "2025-10-17T02:45:08.257942Z",
     "shell.execute_reply.started": "2025-10-17T02:44:50.335125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "→ Loading training data from s3://aai540-ecommerce-recommendation-project-group3/train_user_product_pairs.csv\n",
      "   Training data loaded: (2018753, 17)\n",
      "\n",
      "→ Loading validation data from s3://aai540-ecommerce-recommendation-project-group3/validation_user_product_pairs.csv\n",
      "   Validation data loaded: (1062101, 17)\n",
      "\n",
      " Combined dataset: (3080854, 17)\n",
      "\n",
      "Data loading complete! Ready for EDA.\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE: Load data from S3\n",
    "df_all = load_data_from_s3()\n",
    "\n",
    "print(\"\\nData loading complete! Ready for EDA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37da6894-ceb5-4f61-9e90-5329076533bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:46:37.051606Z",
     "iopub.status.busy": "2025-10-17T02:46:37.051294Z",
     "iopub.status.idle": "2025-10-17T02:46:37.062061Z",
     "shell.execute_reply": "2025-10-17T02:46:37.061411Z",
     "shell.execute_reply.started": "2025-10-17T02:46:37.051579Z"
    }
   },
   "outputs": [],
   "source": [
    "# exploratory data analysis\n",
    "def perform_eda(df):\n",
    "    print(\"\\n1. DATASET OVERVIEW\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Total records: {len(df):,}\")\n",
    "    print(f\"Total features: {len(df.columns)}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "    print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    print(\"\\n2. TARGET VARIABLE ANALYSIS\")\n",
    "    print(\"-\" * 80)\n",
    "    purchase_counts = df['purchased'].value_counts()\n",
    "    purchase_pct = df['purchased'].value_counts(normalize=True) * 100\n",
    "    print(f\"Purchase distribution:\")\n",
    "    print(f\"  Not Purchased (0): {purchase_counts[0]:,} ({purchase_pct[0]:.2f}%)\")\n",
    "    print(f\"  Purchased (1): {purchase_counts[1]:,} ({purchase_pct[1]:.2f}%)\")\n",
    "    print(f\"  Class imbalance ratio: {purchase_counts[0]/purchase_counts[1]:.2f}:1\")\n",
    "    \n",
    "    print(\"\\n3. MISSING VALUES ANALYSIS\")\n",
    "    print(\"-\" * 80)\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Percentage': missing_pct\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "    if len(missing_df) > 0:\n",
    "        print(missing_df)\n",
    "    else:\n",
    "        print(\"  No missing values found!\")\n",
    "    \n",
    "    print(\"\\n4. NUMERICAL FEATURES STATISTICS\")\n",
    "    print(\"-\" * 80)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(df[numeric_cols].describe())\n",
    "    \n",
    "    print(\"\\n5. CATEGORICAL FEATURES ANALYSIS\")\n",
    "    print(\"-\" * 80)\n",
    "    categorical_cols = ['category_code', 'brand']\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Unique values: {df[col].nunique():,}\")\n",
    "            print(f\"  Top 5 categories:\")\n",
    "            print(df[col].value_counts().head())\n",
    "    \n",
    "    print(\"\\n6. USER BEHAVIOR PATTERNS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Unique users: {df['user_id'].nunique():,}\")\n",
    "    print(f\"Unique products: {df['product_id'].nunique():,}\")\n",
    "    print(f\"Avg interactions per user: {df.groupby('user_id')['total_interactions'].sum().mean():.2f}\")\n",
    "    print(f\"Avg price: ${df['price'].mean():.2f}\")\n",
    "    print(f\"Price range: ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\n",
    "    \n",
    "    print(\"\\n7. FEATURE CORRELATIONS WITH TARGET\")\n",
    "    print(\"-\" * 80)\n",
    "    correlations = df[numeric_cols].corrwith(df['purchased']).sort_values(ascending=False)\n",
    "    print(\"Top correlations with purchase:\")\n",
    "    print(correlations.head(10))\n",
    "    \n",
    "    print(\"\\n EDA Complete!\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e01a9715-dbbc-4b51-9a65-415272d113e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:46:39.651182Z",
     "iopub.status.busy": "2025-10-17T02:46:39.650882Z",
     "iopub.status.idle": "2025-10-17T02:46:48.188273Z",
     "shell.execute_reply": "2025-10-17T02:46:48.187491Z",
     "shell.execute_reply.started": "2025-10-17T02:46:39.651156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. DATASET OVERVIEW\n",
      "--------------------------------------------------------------------------------\n",
      "Total records: 3,080,854\n",
      "Total features: 17\n",
      "\n",
      "Columns: ['user_id', 'product_id', 'purchased', 'view_count', 'cart_count', 'total_interactions', 'price', 'category_id', 'category_code', 'brand', 'first_interaction', 'last_interaction', 'product_view_count', 'product_purchase_count', 'product_conversion_rate', 'user_total_events', 'user_total_purchases']\n",
      "\n",
      "Data types:\n",
      "user_id                      int64\n",
      "product_id                   int64\n",
      "purchased                    int64\n",
      "view_count                   int64\n",
      "cart_count                   int64\n",
      "total_interactions           int64\n",
      "price                      float64\n",
      "category_id                  int64\n",
      "category_code               object\n",
      "brand                       object\n",
      "first_interaction           object\n",
      "last_interaction            object\n",
      "product_view_count           int64\n",
      "product_purchase_count       int64\n",
      "product_conversion_rate    float64\n",
      "user_total_events            int64\n",
      "user_total_purchases         int64\n",
      "dtype: object\n",
      "\n",
      "Memory usage: 1070.08 MB\n",
      "\n",
      "2. TARGET VARIABLE ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "Purchase distribution:\n",
      "  Not Purchased (0): 1,203,399 (39.06%)\n",
      "  Purchased (1): 1,877,455 (60.94%)\n",
      "  Class imbalance ratio: 0.64:1\n",
      "\n",
      "3. MISSING VALUES ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "               Missing Count  Percentage\n",
      "category_code         647785   21.026151\n",
      "brand                 284278    9.227247\n",
      "\n",
      "4. NUMERICAL FEATURES STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "            user_id    product_id     purchased    view_count    cart_count  \\\n",
      "count  3.080854e+06  3.080854e+06  3.080854e+06  3.080854e+06  3.080854e+06   \n",
      "mean   5.460140e+08  1.109979e+07  6.093943e-01  1.527109e-01  6.423583e-01   \n",
      "std    2.789266e+07  1.989619e+07  4.878862e-01  3.618553e-01  1.034956e+00   \n",
      "min    1.001409e+08  1.000978e+06  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    5.186768e+08  1.005100e+06  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%    5.439552e+08  3.601448e+06  1.000000e+00  0.000000e+00  0.000000e+00   \n",
      "75%    5.666231e+08  1.270994e+07  1.000000e+00  0.000000e+00  1.000000e+00   \n",
      "max    6.088214e+08  1.000993e+08  1.000000e+00  4.000000e+00  2.160000e+02   \n",
      "\n",
      "       total_interactions         price   category_id  product_view_count  \\\n",
      "count        3.080854e+06  3.080854e+06  3.080854e+06        3.080854e+06   \n",
      "mean         1.604843e+00  2.863672e+02  2.103816e+18        9.424791e+03   \n",
      "std          1.767270e+00  3.360542e+02  7.985535e+16        2.015557e+04   \n",
      "min          1.000000e+00  0.000000e+00  2.053014e+18        1.000000e+00   \n",
      "25%          1.000000e+00  6.950000e+01  2.053014e+18        6.000000e+01   \n",
      "50%          1.000000e+00  1.673700e+02  2.053014e+18        6.470000e+02   \n",
      "75%          2.000000e+00  3.474700e+02  2.232732e+18        6.351000e+03   \n",
      "max          3.530000e+02  2.574070e+03  2.234185e+18        1.040080e+05   \n",
      "\n",
      "       product_purchase_count  product_conversion_rate  user_total_events  \\\n",
      "count            3.080854e+06             3.080854e+06       3.080854e+06   \n",
      "mean             5.246522e+03             4.974473e-01       7.499983e+00   \n",
      "std              1.151532e+04             1.394878e-01       1.733440e+01   \n",
      "min              0.000000e+00             0.000000e+00       1.000000e+00   \n",
      "25%              2.700000e+01             4.588235e-01       1.000000e+00   \n",
      "50%              3.350000e+02             5.183016e-01       3.000000e+00   \n",
      "75%              3.403000e+03             5.636443e-01       7.000000e+00   \n",
      "max              6.126500e+04             1.000000e+00       8.000000e+02   \n",
      "\n",
      "       user_total_purchases  \n",
      "count          3.080854e+06  \n",
      "mean           4.455394e+00  \n",
      "std            1.242610e+01  \n",
      "min            0.000000e+00  \n",
      "25%            0.000000e+00  \n",
      "50%            1.000000e+00  \n",
      "75%            4.000000e+00  \n",
      "max            6.400000e+02  \n",
      "\n",
      "5. CATEGORICAL FEATURES ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "category_code:\n",
      "  Unique values: 135\n",
      "  Top 5 categories:\n",
      "category_code\n",
      "electronics.smartphone         739720\n",
      "construction.tools.light       416095\n",
      "electronics.audio.headphone     96223\n",
      "electronics.clocks              81887\n",
      "electronics.video.tv            73985\n",
      "Name: count, dtype: int64\n",
      "\n",
      "brand:\n",
      "  Unique values: 4,155\n",
      "  Top 5 categories:\n",
      "brand\n",
      "samsung    592474\n",
      "apple      467776\n",
      "xiaomi     247285\n",
      "huawei      91023\n",
      "lucente     50464\n",
      "Name: count, dtype: int64\n",
      "\n",
      "6. USER BEHAVIOR PATTERNS\n",
      "--------------------------------------------------------------------------------\n",
      "Unique users: 1,533,421\n",
      "Unique products: 125,457\n",
      "Avg interactions per user: 3.22\n",
      "Avg price: $286.37\n",
      "Price range: $0.00 - $2574.07\n",
      "\n",
      "7. FEATURE CORRELATIONS WITH TARGET\n",
      "--------------------------------------------------------------------------------\n",
      "Top correlations with purchase:\n",
      "purchased                  1.000000\n",
      "product_conversion_rate    0.311219\n",
      "total_interactions         0.196460\n",
      "user_total_purchases       0.196064\n",
      "user_total_events          0.144444\n",
      "product_view_count         0.102423\n",
      "product_purchase_count     0.101996\n",
      "user_id                    0.033797\n",
      "price                      0.010006\n",
      "category_id               -0.013227\n",
      "dtype: float64\n",
      "\n",
      " EDA Complete!\n",
      "\n",
      " EDA complete! Ready for preprocessing.\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE: Perform Exploratory Data Analysis\n",
    "\n",
    "df_all = perform_eda(df_all)\n",
    "\n",
    "print(\"\\n EDA complete! Ready for preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17384e29-88ed-41fd-8269-7a2b1a71dcc2",
   "metadata": {},
   "source": [
    "## Section 3: Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38601454-8480-4b1a-b27b-dfefaf63a7c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:47:26.144464Z",
     "iopub.status.busy": "2025-10-17T02:47:26.144140Z",
     "iopub.status.idle": "2025-10-17T02:47:26.150653Z",
     "shell.execute_reply": "2025-10-17T02:47:26.149883Z",
     "shell.execute_reply.started": "2025-10-17T02:47:26.144437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Handle missing values and encode categorical features\n",
    "def preprocess_data(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(\"\\n1. Handling Missing Values\")\n",
    "    print(\"-\" * 80)\n",
    "    categorical_cols = ['category_code', 'brand']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        df[col] = df[col].fillna('unknown')\n",
    "        print(f\"✓ {col}: Filled {missing_count:,} missing values ({missing_count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Encode categorical features\n",
    "    print(\"\\n2. Encoding Categorical Features\")\n",
    "    print(\"-\" * 80)\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Unique categories: {df[col].nunique():,}\")\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        df[col + '_encoded'] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "        \n",
    "        print(f\"  ✓ Encoded range: 0 to {df[col + '_encoded'].max()}\")\n",
    "    \n",
    "    print(\"\\n Preprocessing Complete!\")\n",
    "    \n",
    "    return df, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08a39d4b-d9cf-4d16-a4d5-7a111a7d5b80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:47:30.747049Z",
     "iopub.status.busy": "2025-10-17T02:47:30.746776Z",
     "iopub.status.idle": "2025-10-17T02:47:30.753086Z",
     "shell.execute_reply": "2025-10-17T02:47:30.752477Z",
     "shell.execute_reply.started": "2025-10-17T02:47:30.747030Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data: 40% train, 10% test, 10% val, 40% production\n",
    "def split_data_properly(df):\n",
    "    \n",
    "    print(\"\\nSplitting strategy:\")\n",
    "    print(\"  - 40% Training data\")\n",
    "    print(\"  - 10% Test data\")\n",
    "    print(\"  - 10% Validation data\")\n",
    "    print(\"  - 40% Production/Holdout data\")\n",
    "    \n",
    "    # First split: separate 40% for production\n",
    "    df_working, df_production = train_test_split(\n",
    "        df, \n",
    "        test_size=0.40, \n",
    "        random_state=42,\n",
    "        stratify=df['purchased']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n Production data reserved: {len(df_production):,} samples ({len(df_production)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Split remaining 60% into train (40%), test (10%), validation (10%)\n",
    "    df_train, df_temp = train_test_split(\n",
    "        df_working,\n",
    "        test_size=0.3333,\n",
    "        random_state=42,\n",
    "        stratify=df_working['purchased']\n",
    "    )\n",
    "    \n",
    "    df_test, df_val = train_test_split(\n",
    "        df_temp,\n",
    "        test_size=0.5,\n",
    "        random_state=42,\n",
    "        stratify=df_temp['purchased']\n",
    "    )\n",
    "    \n",
    "    print(f\" Training data: {len(df_train):,} samples ({len(df_train)/len(df)*100:.1f}%)\")\n",
    "    print(f\" Test data: {len(df_test):,} samples ({len(df_test)/len(df)*100:.1f}%)\")\n",
    "    print(f\" Validation data: {len(df_val):,} samples ({len(df_val)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Verify class distribution\n",
    "    print(\"\\nVerifying class distribution:\")\n",
    "    for name, dataset in [('Train', df_train), ('Test', df_test), \n",
    "                          ('Validation', df_val), ('Production', df_production)]:\n",
    "        purchase_rate = dataset['purchased'].mean() * 100\n",
    "        print(f\"  {name}: {purchase_rate:.2f}% purchased\")\n",
    "    \n",
    "    return df_train, df_test, df_val, df_production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49059515-f0d9-45a8-8a74-c0c165539ce0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:47:35.399117Z",
     "iopub.status.busy": "2025-10-17T02:47:35.398857Z",
     "iopub.status.idle": "2025-10-17T02:47:43.935179Z",
     "shell.execute_reply": "2025-10-17T02:47:43.934528Z",
     "shell.execute_reply.started": "2025-10-17T02:47:35.399097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Handling Missing Values\n",
      "--------------------------------------------------------------------------------\n",
      "✓ category_code: Filled 647,785 missing values (21.0%)\n",
      "✓ brand: Filled 284,278 missing values (9.2%)\n",
      "\n",
      "2. Encoding Categorical Features\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "category_code:\n",
      "  Unique categories: 136\n",
      "  ✓ Encoded range: 0 to 135\n",
      "\n",
      "brand:\n",
      "  Unique categories: 4,156\n",
      "  ✓ Encoded range: 0 to 4155\n",
      "\n",
      " Preprocessing Complete!\n",
      "\n",
      "Splitting strategy:\n",
      "  - 40% Training data\n",
      "  - 10% Test data\n",
      "  - 10% Validation data\n",
      "  - 40% Production/Holdout data\n",
      "\n",
      " Production data reserved: 1,232,342 samples (40.0%)\n",
      " Training data: 1,232,402 samples (40.0%)\n",
      " Test data: 308,055 samples (10.0%)\n",
      " Validation data: 308,055 samples (10.0%)\n",
      "\n",
      "Verifying class distribution:\n",
      "  Train: 60.94% purchased\n",
      "  Test: 60.94% purchased\n",
      "  Validation: 60.94% purchased\n",
      "  Production: 60.94% purchased\n",
      "\n",
      "Data preprocessing and splitting complete!\n",
      "   - Training set: 1,232,402 records\n",
      "   - Test set: 308,055 records\n",
      "   - Validation set: 308,055 records\n",
      "   - Production set: 1,232,342 records\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE: Preprocess data and create splits\n",
    "df_processed, label_encoders = preprocess_data(df_all)\n",
    "df_train, df_test, df_val, df_prod = split_data_properly(df_processed)\n",
    "\n",
    "print(\"\\nData preprocessing and splitting complete!\")\n",
    "print(f\"   - Training set: {len(df_train):,} records\")\n",
    "print(f\"   - Test set: {len(df_test):,} records\")\n",
    "print(f\"   - Validation set: {len(df_val):,} records\")\n",
    "print(f\"   - Production set: {len(df_prod):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c38b76-7425-411f-8d14-7bcb468ef28a",
   "metadata": {},
   "source": [
    "## Section 4: Feature Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e119e5a-8526-4ffa-81ba-f9ce7a01c376",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:47:48.955519Z",
     "iopub.status.busy": "2025-10-17T02:47:48.955123Z",
     "iopub.status.idle": "2025-10-17T02:47:48.963814Z",
     "shell.execute_reply": "2025-10-17T02:47:48.963095Z",
     "shell.execute_reply.started": "2025-10-17T02:47:48.955491Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize SageMaker Feature Store and design feature groups\n",
    "def create_feature_store(df_sample):\n",
    "    feature_group_name = f\"ecommerce-user-product-features-{int(time.time())}\"\n",
    "    \n",
    "    print(f\"\\n→ Creating Feature Group: {feature_group_name}\")\n",
    "    \n",
    "    # Prepare dataframe for feature store\n",
    "    df_fs = df_sample.copy()\n",
    "    df_fs['event_time'] = pd.Timestamp.now().isoformat()\n",
    "    df_fs['record_id'] = df_fs.index.astype(str) + '_' + df_fs['user_id'].astype(str)\n",
    "    \n",
    "    # Select features for feature store\n",
    "    feature_columns = [\n",
    "        'record_id', 'event_time', 'user_id', 'product_id', 'purchased',\n",
    "        'view_count', 'cart_count', 'total_interactions', 'price',\n",
    "        'category_id', 'category_code_encoded', 'brand_encoded',\n",
    "        'product_view_count', 'product_purchase_count',\n",
    "        'product_conversion_rate', 'user_total_events', 'user_total_purchases'\n",
    "    ]\n",
    "    \n",
    "    df_fs = df_fs[feature_columns]\n",
    "    \n",
    "    # Initialize Feature Group\n",
    "    feature_group = FeatureGroup(\n",
    "        name=feature_group_name,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    \n",
    "    print(\"\\n Feature Group Design:\")\n",
    "    print(f\"  Name: {feature_group_name}\")\n",
    "    print(f\"  Record Identifier: record_id\")\n",
    "    print(f\"  Event Time Feature: event_time\")\n",
    "    print(f\"  Number of features: {len(feature_columns)}\")\n",
    "    \n",
    "    # Load feature definitions\n",
    "    feature_group.load_feature_definitions(data_frame=df_fs)\n",
    "    \n",
    "    print(f\"\\nFeatures in group:\")\n",
    "    for i, col in enumerate(feature_columns, 1):\n",
    "        dtype = df_fs[col].dtype\n",
    "        print(f\"  {i:2d}. {col:30s} ({dtype})\")\n",
    "    \n",
    "    # Create feature group\n",
    "    try:\n",
    "        feature_group.create(\n",
    "            s3_uri=f\"s3://{BUCKET_NAME}/{FEATURE_STORE_PREFIX}\",\n",
    "            record_identifier_name='record_id',\n",
    "            event_time_feature_name='event_time',\n",
    "            role_arn=role,\n",
    "            enable_online_store=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\n Feature group created successfully!\")\n",
    "        print(f\"  Online store: Enabled\")\n",
    "        print(f\"  Offline store: s3://{BUCKET_NAME}/{FEATURE_STORE_PREFIX}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nNote: {str(e)}\")\n",
    "    \n",
    "    return feature_group, feature_group_name, df_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d0abd58-7721-4cfa-b32d-34b847cde40a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:47:54.423093Z",
     "iopub.status.busy": "2025-10-17T02:47:54.422792Z",
     "iopub.status.idle": "2025-10-17T02:48:39.862149Z",
     "shell.execute_reply": "2025-10-17T02:48:39.861412Z",
     "shell.execute_reply.started": "2025-10-17T02:47:54.423066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Production data saved: s3://aai540-ecommerce-recommendation-project-group3/mlops-pipeline/production_data.csv\n",
      " Train saved: s3://aai540-ecommerce-recommendation-project-group3/mlops-pipeline/train_processed.csv\n",
      " Test saved: s3://aai540-ecommerce-recommendation-project-group3/mlops-pipeline/test_processed.csv\n",
      " Validation saved: s3://aai540-ecommerce-recommendation-project-group3/mlops-pipeline/validation_processed.csv\n",
      "\n",
      " All splits saved successfully!\n",
      "\n",
      "→ Creating Feature Group: ecommerce-user-product-features-1760669319\n",
      "\n",
      " Feature Group Design:\n",
      "  Name: ecommerce-user-product-features-1760669319\n",
      "  Record Identifier: record_id\n",
      "  Event Time Feature: event_time\n",
      "  Number of features: 17\n",
      "\n",
      "Features in group:\n",
      "   1. record_id                      (object)\n",
      "   2. event_time                     (object)\n",
      "   3. user_id                        (int64)\n",
      "   4. product_id                     (int64)\n",
      "   5. purchased                      (int64)\n",
      "   6. view_count                     (int64)\n",
      "   7. cart_count                     (int64)\n",
      "   8. total_interactions             (int64)\n",
      "   9. price                          (float64)\n",
      "  10. category_id                    (int64)\n",
      "  11. category_code_encoded          (int64)\n",
      "  12. brand_encoded                  (int64)\n",
      "  13. product_view_count             (int64)\n",
      "  14. product_purchase_count         (int64)\n",
      "  15. product_conversion_rate        (float64)\n",
      "  16. user_total_events              (int64)\n",
      "  17. user_total_purchases           (int64)\n",
      "\n",
      " Feature group created successfully!\n",
      "  Online store: Enabled\n",
      "  Offline store: s3://aai540-ecommerce-recommendation-project-group3/mlops-pipeline/feature-store/\n",
      "\n",
      " Feature Store setup complete!\n",
      "   Feature Group: ecommerce-user-product-features-1760669319\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE: Save splits to S3 and create Feature Store\n",
    "save_splits_to_s3(df_train, df_test, df_val, df_prod)\n",
    "\n",
    "feature_group, feature_group_name, df_fs_sample = create_feature_store(df_train.head(100))\n",
    "\n",
    "print(\"\\n Feature Store setup complete!\")\n",
    "print(f\"   Feature Group: {feature_group_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb3472-1d8e-4e95-8e74-d0815b2a776d",
   "metadata": {},
   "source": [
    "## Section 5: Benchmark Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55a7d1ff-5c3a-4bec-9540-3249183c97eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:51:45.360577Z",
     "iopub.status.busy": "2025-10-17T02:51:45.360312Z",
     "iopub.status.idle": "2025-10-17T02:51:45.365670Z",
     "shell.execute_reply": "2025-10-17T02:51:45.365003Z",
     "shell.execute_reply.started": "2025-10-17T02:51:45.360558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created/verified 'code' directory at: /home/sagemaker-user/code\n",
      " Write permissions verified\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create code directory manually\n",
    "try:\n",
    "    os.makedirs('code', exist_ok=True)\n",
    "    print(f\"✓ Created/verified 'code' directory at: {os.path.join(os.getcwd(), 'code')}\")\n",
    "    \n",
    "    # Check write permissions\n",
    "    test_file = 'code/.test_write'\n",
    "    try:\n",
    "        with open(test_file, 'w') as f:\n",
    "            f.write('test')\n",
    "        os.remove(test_file)\n",
    "        print(\" Write permissions verified\")\n",
    "    except Exception as e:\n",
    "        print(f\" Write permission error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\" Directory creation error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c1080cc-798f-4772-ac97-ffbd3a911563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:56:27.171305Z",
     "iopub.status.busy": "2025-10-17T02:56:27.171048Z",
     "iopub.status.idle": "2025-10-17T02:56:27.176083Z",
     "shell.execute_reply": "2025-10-17T02:56:27.175420Z",
     "shell.execute_reply.started": "2025-10-17T02:56:27.171284Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save all data splits to S3\n",
    "def create_preprocessing_script():\n",
    "    \n",
    "    # Save production data\n",
    "    prod_key = f\"{prefix}/production_data.csv\"\n",
    "    csv_buffer = io.StringIO()\n",
    "    df_prod.to_csv(csv_buffer, index=False)\n",
    "    s3_client.put_object(\n",
    "        Bucket=bucket,\n",
    "        Key=prod_key,\n",
    "        Body=csv_buffer.getvalue()\n",
    "    )\n",
    "    print(f\" Production data saved: s3://{bucket}/{prod_key}\")\n",
    "    \n",
    "    # Save other splits\n",
    "    for name, data in [('train', df_train), ('test', df_test), ('validation', df_val)]:\n",
    "        key = f\"{prefix}/{name}_processed.csv\"\n",
    "        csv_buffer = io.StringIO()\n",
    "        data.to_csv(csv_buffer, index=False)\n",
    "        s3_client.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())\n",
    "        print(f\"✓ {name.capitalize()} saved: s3://{bucket}/{key}\")\n",
    "    \n",
    "    print(\"\\n All splits saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a6e3cd1-6839-418c-9f96-63497ead4388",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:59:46.653737Z",
     "iopub.status.busy": "2025-10-17T02:59:46.653408Z",
     "iopub.status.idle": "2025-10-17T02:59:46.660374Z",
     "shell.execute_reply": "2025-10-17T02:59:46.659828Z",
     "shell.execute_reply.started": "2025-10-17T02:59:46.653710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created code/preprocess.py (4837 bytes)\n"
     ]
    }
   ],
   "source": [
    "preprocess_content = '''import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def target_encode(train_series, test_series, target_series, min_samples_leaf=1, smoothing=1):\n",
    "    temp = pd.concat([train_series, target_series], axis=1)\n",
    "    averages = temp.groupby(train_series.name)[target_series.name].agg([\"mean\", \"count\"])\n",
    "    smoothing_component = target_series.mean()\n",
    "    averages[\"smoothed\"] = (averages[\"mean\"] * averages[\"count\"] + smoothing_component * smoothing) / (averages[\"count\"] + smoothing)\n",
    "    mapping = averages[\"smoothed\"]\n",
    "    train_encoded = train_series.map(mapping)\n",
    "    test_encoded = test_series.map(mapping)\n",
    "    test_encoded.fillna(smoothing_component, inplace=True)\n",
    "    return train_encoded, test_encoded\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-input\", type=str)\n",
    "    parser.add_argument(\"--test-input\", type=str, default=None)\n",
    "    parser.add_argument(\"--validation-input\", type=str, default=None)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    train_input_path = os.path.join(base_dir, \"input/train\", args.train_input)\n",
    "    \n",
    "    print(\"Loading training data...\")\n",
    "    df_train = pd.read_csv(train_input_path)\n",
    "    \n",
    "    df_test = None\n",
    "    if args.test_input:\n",
    "        test_input_path = os.path.join(base_dir, \"input/test\", args.test_input)\n",
    "        if os.path.exists(test_input_path):\n",
    "            print(\"Loading test data...\")\n",
    "            df_test = pd.read_csv(test_input_path)\n",
    "    \n",
    "    df_val = None\n",
    "    if args.validation_input:\n",
    "        val_input_path = os.path.join(base_dir, \"input/validation\", args.validation_input)\n",
    "        if os.path.exists(val_input_path):\n",
    "            print(\"Loading validation data...\")\n",
    "            df_val = pd.read_csv(val_input_path)\n",
    "\n",
    "    print(\"Applying target encoding...\")\n",
    "    categorical_cols = ['brand', 'category_code']\n",
    "    target_column = 'purchased'\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        df_train[col] = df_train[col].fillna('unknown')\n",
    "        train_encoded, _ = target_encode(\n",
    "            df_train[col], \n",
    "            df_train[col], \n",
    "            df_train[target_column]\n",
    "        )\n",
    "        df_train[f\"{col}_encoded\"] = train_encoded\n",
    "        \n",
    "        if df_test is not None:\n",
    "            df_test[col] = df_test[col].fillna('unknown')\n",
    "            _, test_encoded = target_encode(\n",
    "                df_train[col],\n",
    "                df_test[col],\n",
    "                df_train[target_column]\n",
    "            )\n",
    "            df_test[f\"{col}_encoded\"] = test_encoded\n",
    "        \n",
    "        if df_val is not None:\n",
    "            df_val[col] = df_val[col].fillna('unknown')\n",
    "            _, val_encoded = target_encode(\n",
    "                df_train[col],\n",
    "                df_val[col],\n",
    "                df_train[target_column]\n",
    "            )\n",
    "            df_val[f\"{col}_encoded\"] = val_encoded\n",
    "\n",
    "    print(\"Cleaning columns...\")\n",
    "    columns_to_drop = ['user_id', 'product_id', 'first_interaction', 'last_interaction'] + categorical_cols\n",
    "    df_train_processed = df_train.drop(columns=columns_to_drop)\n",
    "    \n",
    "    feature_columns = [col for col in df_train_processed.columns if col != target_column]\n",
    "    final_train_df = df_train_processed[[target_column] + feature_columns]\n",
    "\n",
    "    print(\"Saving processed training data...\")\n",
    "    train_output_path = os.path.join(base_dir, \"output/train/train_commerce.csv\")\n",
    "    os.makedirs(os.path.dirname(train_output_path), exist_ok=True)\n",
    "    final_train_df.to_csv(train_output_path, header=False, index=False)\n",
    "    \n",
    "    if df_test is not None:\n",
    "        df_test_processed = df_test.drop(columns=columns_to_drop)\n",
    "        final_test_df = df_test_processed[[target_column] + feature_columns]\n",
    "        test_output_path = os.path.join(base_dir, \"output/test/test_commerce.csv\")\n",
    "        os.makedirs(os.path.dirname(test_output_path), exist_ok=True)\n",
    "        final_test_df.to_csv(test_output_path, header=False, index=False)\n",
    "        print(\"Saved processed test data\")\n",
    "    \n",
    "    if df_val is not None:\n",
    "        df_val_processed = df_val.drop(columns=columns_to_drop)\n",
    "        final_val_df = df_val_processed[[target_column] + feature_columns]\n",
    "        \n",
    "        rand_split = np.random.rand(len(final_val_df))\n",
    "        new_validation_set = final_val_df[rand_split < 0.5]\n",
    "        batch_set = final_val_df[rand_split >= 0.5]\n",
    "        \n",
    "        validation_output_path = os.path.join(base_dir, \"output/validation/validation_commerce.csv\")\n",
    "        batch_output_path = os.path.join(base_dir, \"output/batch/batch_commerce.csv\")\n",
    "        \n",
    "        os.makedirs(os.path.dirname(validation_output_path), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(batch_output_path), exist_ok=True)\n",
    "        \n",
    "        new_validation_set.to_csv(validation_output_path, header=False, index=False)\n",
    "        batch_set.to_csv(batch_output_path, header=False, index=False)\n",
    "        print(\"Saved processed validation and batch data\")\n",
    "\n",
    "    print(\"Preprocessing complete!\")\n",
    "'''\n",
    "\n",
    "with open('code/preprocess.py', 'w') as f:\n",
    "    f.write(preprocess_content)\n",
    "\n",
    "print(\"✓ Created code/preprocess.py ({} bytes)\".format(os.path.getsize('code/preprocess.py')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59fb0395-0197-4eec-a6b6-0d0ac271834f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T03:00:10.091135Z",
     "iopub.status.busy": "2025-10-17T03:00:10.090855Z",
     "iopub.status.idle": "2025-10-17T03:00:10.096328Z",
     "shell.execute_reply": "2025-10-17T03:00:10.095730Z",
     "shell.execute_reply.started": "2025-10-17T03:00:10.091116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created code/evaluation.py (2171 bytes)\n"
     ]
    }
   ],
   "source": [
    "evaluation_content = '''import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "    \n",
    "    model = xgb.Booster()\n",
    "    model.load_model(\"xgboost-model\")\n",
    "    \n",
    "    test_path = \"/opt/ml/processing/test/test_commerce.csv\"\n",
    "    df_test = pd.read_csv(test_path, header=None)\n",
    "    \n",
    "    y_test = df_test.iloc[:, 0].values\n",
    "    X_test = df_test.iloc[:, 1:].values\n",
    "    \n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    predictions_prob = model.predict(dtest)\n",
    "    predictions = (predictions_prob > 0.5).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, zero_division=0)\n",
    "    recall = recall_score(y_test, predictions, zero_division=0)\n",
    "    f1 = f1_score(y_test, predictions, zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, predictions_prob)\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    report = {\n",
    "        \"binary_classification_metrics\": {\n",
    "            \"accuracy\": {\"value\": float(accuracy)},\n",
    "            \"precision\": {\"value\": float(precision)},\n",
    "            \"recall\": {\"value\": float(recall)},\n",
    "            \"f1_score\": {\"value\": float(f1)},\n",
    "            \"auc\": {\"value\": float(auc)},\n",
    "            \"confusion_matrix\": {\n",
    "                \"true_negatives\": int(tn),\n",
    "                \"false_positives\": int(fp),\n",
    "                \"false_negatives\": int(fn),\n",
    "                \"true_positives\": int(tp)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    print(\"Evaluation complete!\")\n",
    "    print(json.dumps(report, indent=4))\n",
    "'''\n",
    "\n",
    "with open('code/evaluation.py', 'w') as f:\n",
    "    f.write(evaluation_content)\n",
    "\n",
    "print(\"✓ Created code/evaluation.py ({} bytes)\".format(os.path.getsize('code/evaluation.py')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a9af22d-d194-4111-826f-168167f07abc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T03:00:38.635112Z",
     "iopub.status.busy": "2025-10-17T03:00:38.634854Z",
     "iopub.status.idle": "2025-10-17T03:00:38.640686Z",
     "shell.execute_reply": "2025-10-17T03:00:38.640100Z",
     "shell.execute_reply.started": "2025-10-17T03:00:38.635093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " preprocess.py exists (4,837 bytes)\n",
      " evaluation.py exists (2,171 bytes)\n",
      "\n",
      "Contents of code/ directory:\n",
      "  - preprocess.py (4,837 bytes)\n",
      "  - evaluation.py (2,171 bytes)\n",
      "\n",
      " Scripts created successfully! You can now run Section 6.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('code/preprocess.py'):\n",
    "    size = os.path.getsize('code/preprocess.py')\n",
    "    print(f\" preprocess.py exists ({size:,} bytes)\")\n",
    "else:\n",
    "    print(\" preprocess.py NOT FOUND\")\n",
    "\n",
    "if os.path.exists('code/evaluation.py'):\n",
    "    size = os.path.getsize('code/evaluation.py')\n",
    "    print(f\" evaluation.py exists ({size:,} bytes)\")\n",
    "else:\n",
    "    print(\" evaluation.py NOT FOUND\")\n",
    "\n",
    "print(\"\\nContents of code/ directory:\")\n",
    "for item in os.listdir('code'):\n",
    "    item_path = os.path.join('code', item)\n",
    "    if os.path.isfile(item_path):\n",
    "        size = os.path.getsize(item_path)\n",
    "        print(f\"  - {item} ({size:,} bytes)\")\n",
    "\n",
    "print(\"\\n Scripts created successfully! You can now run Section 6.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f119d-54ef-4691-96f0-77ccb3710dbb",
   "metadata": {},
   "source": [
    "## Section 6: Model Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93977fe9-d3ae-440a-a7d4-43b20d7c0024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:21:17.140320Z",
     "iopub.status.busy": "2025-10-17T02:21:17.140016Z",
     "iopub.status.idle": "2025-10-17T02:21:17.146625Z",
     "shell.execute_reply": "2025-10-17T02:21:17.145871Z",
     "shell.execute_reply.started": "2025-10-17T02:21:17.140294Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run SageMaker Processing job for data preprocessing\n",
    "def run_preprocessing_job(train_path, test_path, val_path):\n",
    "    \n",
    "    sklearn_processor = SKLearnProcessor(\n",
    "        framework_version=\"1.2-1\",\n",
    "        role=role,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        instance_count=1,\n",
    "        base_job_name=\"ecommerce-preprocessing\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n→ Starting SageMaker Processing Job...\")\n",
    "    sklearn_processor.run(\n",
    "        code=\"code/preprocess.py\",\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=train_path,\n",
    "                destination=\"/opt/ml/processing/input/train\",\n",
    "                s3_data_distribution_type=\"FullyReplicated\"\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=test_path,\n",
    "                destination=\"/opt/ml/processing/input/test\",\n",
    "                s3_data_distribution_type=\"FullyReplicated\"\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=val_path,\n",
    "                destination=\"/opt/ml/processing/input/validation\",\n",
    "                s3_data_distribution_type=\"FullyReplicated\"\n",
    "            )\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                source=\"/opt/ml/processing/output/train\",\n",
    "                destination=f\"s3://{default_bucket}/{S3_PREFIX}/processed/train\"\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                source=\"/opt/ml/processing/output/test\",\n",
    "                destination=f\"s3://{default_bucket}/{S3_PREFIX}/processed/test\"\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                source=\"/opt/ml/processing/output/validation\",\n",
    "                destination=f\"s3://{default_bucket}/{S3_PREFIX}/processed/validation\"\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                source=\"/opt/ml/processing/output/batch\",\n",
    "                destination=f\"s3://{default_bucket}/{S3_PREFIX}/processed/batch\"\n",
    "            )\n",
    "        ],\n",
    "        arguments=[\n",
    "            \"--train-input\", \"train_processed.csv\",\n",
    "            \"--test-input\", \"test_processed.csv\",\n",
    "            \"--validation-input\", \"validation_processed.csv\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"\\n Preprocessing job completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34fcbdec-731f-4607-b999-0166555ffbe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:21:42.124249Z",
     "iopub.status.busy": "2025-10-17T02:21:42.123946Z",
     "iopub.status.idle": "2025-10-17T02:21:42.130727Z",
     "shell.execute_reply": "2025-10-17T02:21:42.129868Z",
     "shell.execute_reply.started": "2025-10-17T02:21:42.124223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train XGBoost model with SageMaker\n",
    "def train_xgboost_model():\n",
    "    \n",
    "    job_name = \"xgb-recommender-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "    output_location = f\"s3://{default_bucket}/{S3_PREFIX}/output/{job_name}\"\n",
    "    \n",
    "    image = sagemaker.image_uris.retrieve(\n",
    "        framework=\"xgboost\",\n",
    "        region=region,\n",
    "        version=\"1.7-1\"\n",
    "    )\n",
    "    \n",
    "    estimator = sagemaker.estimator.Estimator(\n",
    "        image,\n",
    "        role,\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        volume_size=50,\n",
    "        input_mode=\"File\",\n",
    "        output_path=output_location,\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    \n",
    "    estimator.set_hyperparameters(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\",\n",
    "        max_depth=5,\n",
    "        eta=0.2,\n",
    "        gamma=4,\n",
    "        min_child_weight=6,\n",
    "        subsample=0.8,\n",
    "        verbosity=0,\n",
    "        num_round=100,\n",
    "    )\n",
    "    \n",
    "    # Define training data\n",
    "    train_data = TrainingInput(\n",
    "        f\"s3://{default_bucket}/{S3_PREFIX}/processed/train\",\n",
    "        content_type=\"text/csv\"\n",
    "    )\n",
    "    \n",
    "    validation_data = TrainingInput(\n",
    "        f\"s3://{default_bucket}/{S3_PREFIX}/processed/validation\",\n",
    "        content_type=\"text/csv\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n→ Starting training job: {job_name}\")\n",
    "    estimator.fit(\n",
    "        inputs={\"train\": train_data, \"validation\": validation_data},\n",
    "        job_name=job_name,\n",
    "        logs=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n Training completed successfully!\")\n",
    "    return estimator, job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41882237-8e64-4b20-b5c5-d782850ac902",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T03:00:51.620409Z",
     "iopub.status.busy": "2025-10-17T03:00:51.620112Z",
     "iopub.status.idle": "2025-10-17T03:06:18.229112Z",
     "shell.execute_reply": "2025-10-17T03:06:18.228481Z",
     "shell.execute_reply.started": "2025-10-17T03:00:51.620384Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "→ Starting SageMaker Processing Job...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name ecommerce-preprocessing-2025-10-17-03-00-51-682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............\u001b[34mLoading training data...\u001b[0m\n",
      "\u001b[34mLoading test data...\u001b[0m\n",
      "\u001b[34mLoading validation data...\u001b[0m\n",
      "\u001b[34mApplying target encoding...\u001b[0m\n",
      "\u001b[34mCleaning columns...\u001b[0m\n",
      "\u001b[34mSaving processed training data...\u001b[0m\n",
      "\u001b[34mSaved processed test data\u001b[0m\n",
      "\u001b[34mSaved processed validation and batch data\u001b[0m\n",
      "\u001b[34mPreprocessing complete!\u001b[0m\n",
      "\n",
      "\n",
      " Preprocessing job completed successfully!\n",
      "\n",
      " Preprocessing job complete!\n",
      "   Processed data available at: s3://sagemaker-us-east-1-115718999037/mlops-pipeline/processed/\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE: Run preprocessing job\n",
    "# Verify the preprocessing script exists\n",
    "import os\n",
    "if not os.path.exists('code/preprocess.py'):\n",
    "    print(\"\\n ERROR: code/preprocess.py not found!\")\n",
    "    print(\"Please run Section 5 first to create the preprocessing script.\")\n",
    "    raise FileNotFoundError(\"code/preprocess.py must exist before running preprocessing job\")\n",
    "\n",
    "train_s3_path = f\"s3://{BUCKET_NAME}/{S3_PREFIX}/train_processed.csv\"\n",
    "test_s3_path = f\"s3://{BUCKET_NAME}/{S3_PREFIX}/test_processed.csv\"\n",
    "val_s3_path = f\"s3://{BUCKET_NAME}/{S3_PREFIX}/validation_processed.csv\"\n",
    "\n",
    "run_preprocessing_job(train_s3_path, test_s3_path, val_s3_path)\n",
    "\n",
    "print(\"\\n Preprocessing job complete!\")\n",
    "print(f\"   Processed data available at: s3://{default_bucket}/{S3_PREFIX}/processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a76a4bed-d4b3-4297-ac1d-acd97cbde84e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T03:06:28.871503Z",
     "iopub.status.busy": "2025-10-17T03:06:28.871205Z",
     "iopub.status.idle": "2025-10-17T03:10:24.037458Z",
     "shell.execute_reply": "2025-10-17T03:10:24.036747Z",
     "shell.execute_reply.started": "2025-10-17T03:06:28.871476Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: xgb-recommender-2025-10-17-03-06-28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "→ Starting training job: xgb-recommender-2025-10-17-03-06-28\n",
      "2025-10-17 03:06:30 Starting - Starting the training job...\n",
      "2025-10-17 03:07:04 Downloading - Downloading input data...\n",
      "2025-10-17 03:07:34 Downloading - Downloading the training image......\n",
      "2025-10-17 03:08:20 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:08:32.551 ip-10-0-180-38.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:08:32.612 ip-10-0-180-38.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:32:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:32:INFO] Failed to parse hyperparameter eval_metric value auc to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:32:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:32:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:32:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:32:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:32:INFO] File path /opt/ml/input/data/train of input files\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:32:INFO] Making smlinks from folder /opt/ml/input/data/train to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:32:INFO] creating symlink between Path /opt/ml/input/data/train/train_commerce.csv and destination /tmp/sagemaker_xgboost_input_data/train_commerce.csv-478919458282151975\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:32:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:33:INFO] File path /opt/ml/input/data/validation of input files\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:33:INFO] Making smlinks from folder /opt/ml/input/data/validation to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:33:INFO] creating symlink between Path /opt/ml/input/data/validation/validation_commerce.csv and destination /tmp/sagemaker_xgboost_input_data/validation_commerce.csv1549269329416513182\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:33:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:33:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:33:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:33:INFO] Train matrix has 1232402 rows and 12 columns\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:33:INFO] Validation matrix has 154080 rows\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:08:33.982 ip-10-0-180-38.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:08:33.982 ip-10-0-180-38.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:08:33.983 ip-10-0-180-38.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:08:33.983 ip-10-0-180-38.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:08:33:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:08:35.605 ip-10-0-180-38.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:08:35.609 ip-10-0-180-38.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.99825#011validation-auc:0.99823\u001b[0m\n",
      "\u001b[34m[1]#011train-auc:0.99825#011validation-auc:0.99823\u001b[0m\n",
      "\u001b[34m[2]#011train-auc:0.99825#011validation-auc:0.99823\u001b[0m\n",
      "\u001b[34m[3]#011train-auc:0.99825#011validation-auc:0.99823\u001b[0m\n",
      "\u001b[34m[4]#011train-auc:0.99825#011validation-auc:0.99823\u001b[0m\n",
      "\u001b[34m[5]#011train-auc:0.99825#011validation-auc:0.99823\u001b[0m\n",
      "\u001b[34m[6]#011train-auc:0.99825#011validation-auc:0.99823\u001b[0m\n",
      "\u001b[34m[7]#011train-auc:0.99832#011validation-auc:0.99828\u001b[0m\n",
      "\u001b[34m[8]#011train-auc:0.99832#011validation-auc:0.99828\u001b[0m\n",
      "\u001b[34m[9]#011train-auc:0.99832#011validation-auc:0.99828\u001b[0m\n",
      "\u001b[34m[10]#011train-auc:0.99979#011validation-auc:0.99979\u001b[0m\n",
      "\u001b[34m[11]#011train-auc:0.99979#011validation-auc:0.99979\u001b[0m\n",
      "\u001b[34m[12]#011train-auc:0.99979#011validation-auc:0.99979\u001b[0m\n",
      "\u001b[34m[13]#011train-auc:0.99979#011validation-auc:0.99979\u001b[0m\n",
      "\u001b[34m[14]#011train-auc:0.99979#011validation-auc:0.99979\u001b[0m\n",
      "\u001b[34m[15]#011train-auc:0.99979#011validation-auc:0.99979\u001b[0m\n",
      "\u001b[34m[16]#011train-auc:0.99989#011validation-auc:0.99988\u001b[0m\n",
      "\u001b[34m[17]#011train-auc:0.99989#011validation-auc:0.99988\u001b[0m\n",
      "\u001b[34m[18]#011train-auc:0.99989#011validation-auc:0.99988\u001b[0m\n",
      "\u001b[34m[19]#011train-auc:0.99996#011validation-auc:0.99995\u001b[0m\n",
      "\u001b[34m[20]#011train-auc:0.99996#011validation-auc:0.99995\u001b[0m\n",
      "\u001b[34m[21]#011train-auc:0.99997#011validation-auc:0.99996\u001b[0m\n",
      "\u001b[34m[22]#011train-auc:0.99998#011validation-auc:0.99998\u001b[0m\n",
      "\u001b[34m[23]#011train-auc:0.99998#011validation-auc:0.99998\u001b[0m\n",
      "\u001b[34m[24]#011train-auc:0.99998#011validation-auc:0.99998\u001b[0m\n",
      "\u001b[34m[25]#011train-auc:0.99998#011validation-auc:0.99998\u001b[0m\n",
      "\u001b[34m[26]#011train-auc:0.99998#011validation-auc:0.99998\u001b[0m\n",
      "\u001b[34m[27]#011train-auc:0.99998#011validation-auc:0.99998\u001b[0m\n",
      "\u001b[34m[28]#011train-auc:0.99998#011validation-auc:0.99998\u001b[0m\n",
      "\u001b[34m[29]#011train-auc:0.99999#011validation-auc:0.99998\u001b[0m\n",
      "\u001b[34m[30]#011train-auc:0.99999#011validation-auc:0.99998\u001b[0m\n",
      "\u001b[34m[31]#011train-auc:0.99999#011validation-auc:0.99999\u001b[0m\n",
      "\u001b[34m[32]#011train-auc:0.99999#011validation-auc:0.99999\u001b[0m\n",
      "\u001b[34m[33]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[34]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[35]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[36]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[37]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[38]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[39]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[40]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[41]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[42]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[43]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[44]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[45]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[46]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[47]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[48]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[49]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[50]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[51]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[52]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[53]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[54]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[55]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[56]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[57]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[58]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[59]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[60]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[61]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[62]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[63]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[64]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[65]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[66]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[67]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[68]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[69]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[70]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[71]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[72]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[73]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[74]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[75]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[76]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[77]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[78]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[79]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[80]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[81]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[82]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[83]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[84]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[85]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[86]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[87]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[88]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[89]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[90]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[91]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[92]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\n",
      "2025-10-17 03:09:40 Uploading - Uploading generated training model\u001b[34m[93]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[94]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[95]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[96]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[97]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[98]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\u001b[34m[99]#011train-auc:1.00000#011validation-auc:1.00000\u001b[0m\n",
      "\n",
      "2025-10-17 03:09:53 Completed - Training job completed\n",
      "Training seconds: 170\n",
      "Billable seconds: 170\n",
      "\n",
      " Training completed successfully!\n",
      "\n",
      " Model training complete!\n",
      "   Job Name: xgb-recommender-2025-10-17-03-06-28\n",
      "   Model artifacts: s3://sagemaker-us-east-1-115718999037/mlops-pipeline/output/xgb-recommender-2025-10-17-03-06-28/xgb-recommender-2025-10-17-03-06-28/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE: Train XGBoost model\n",
    "trained_estimator, training_job_name = train_xgboost_model()\n",
    "\n",
    "print(\"\\n Model training complete!\")\n",
    "print(f\"   Job Name: {training_job_name}\")\n",
    "print(f\"   Model artifacts: {trained_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec34cf-34e8-4b05-aefe-a9087ba6cf64",
   "metadata": {},
   "source": [
    "## Section 7: Model Deployment and Batch Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0e15912c-8bc9-4162-8855-b862e081542e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T03:10:33.651420Z",
     "iopub.status.busy": "2025-10-17T03:10:33.651118Z",
     "iopub.status.idle": "2025-10-17T03:10:33.656565Z",
     "shell.execute_reply": "2025-10-17T03:10:33.655770Z",
     "shell.execute_reply.started": "2025-10-17T03:10:33.651393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Deploy model for batch transformation\n",
    "def deploy_batch_transform(estimator, model_name):\n",
    "    \n",
    "    transformer = estimator.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        output_path=f\"s3://{default_bucket}/{S3_PREFIX}/batch-output\"\n",
    "    )\n",
    "    \n",
    "    batch_input = f\"s3://{default_bucket}/{S3_PREFIX}/processed/batch\"\n",
    "    \n",
    "    print(f\"\\n Starting batch transform job...\")\n",
    "    print(f\"  Input: {batch_input}\")\n",
    "    print(f\"  Output: {transformer.output_path}\")\n",
    "    \n",
    "    transformer.transform(\n",
    "        data=batch_input,\n",
    "        content_type=\"text/csv\",\n",
    "        split_type=\"Line\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n Waiting for transform job to complete...\")\n",
    "    transformer.wait()\n",
    "    \n",
    "    print(\"\\n Batch transform completed successfully!\")\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b772de7-0f29-433c-81a6-05185c99b9f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T03:19:21.191009Z",
     "iopub.status.busy": "2025-10-17T03:19:21.190753Z",
     "iopub.status.idle": "2025-10-17T03:19:24.299421Z",
     "shell.execute_reply": "2025-10-17T03:19:24.298660Z",
     "shell.execute_reply.started": "2025-10-17T03:19:21.190988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batch shape: (153975, 13)\n",
      "Fixed batch shape: (153975, 12)\n",
      " Batch file fixed! Now re-run Section 7.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "# Read the current batch file\n",
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(\n",
    "    Bucket=default_bucket, \n",
    "    Key=f'{S3_PREFIX}/processed/batch/batch_commerce.csv'\n",
    ")\n",
    "batch_df = pd.read_csv(io.BytesIO(obj['Body'].read()), header=None)\n",
    "\n",
    "print(f\"Current batch shape: {batch_df.shape}\")\n",
    "\n",
    "# Remove first column (target variable)\n",
    "batch_df_fixed = batch_df.iloc[:, 1:]\n",
    "print(f\"Fixed batch shape: {batch_df_fixed.shape}\")\n",
    "\n",
    "# Save back to S3\n",
    "csv_buffer = io.StringIO()\n",
    "batch_df_fixed.to_csv(csv_buffer, header=False, index=False)\n",
    "s3_client.put_object(\n",
    "    Bucket=default_bucket,\n",
    "    Key=f'{S3_PREFIX}/processed/batch/batch_commerce.csv',\n",
    "    Body=csv_buffer.getvalue()\n",
    ")\n",
    "\n",
    "print(\" Batch file fixed! Now re-run Section 7.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "552af8a6-fcbb-42e3-811c-a892ded67882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T03:19:27.031253Z",
     "iopub.status.busy": "2025-10-17T03:19:27.030951Z",
     "iopub.status.idle": "2025-10-17T03:25:32.325507Z",
     "shell.execute_reply": "2025-10-17T03:25:32.324699Z",
     "shell.execute_reply.started": "2025-10-17T03:19:27.031228Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2025-10-17-03-19-27-032\n",
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2025-10-17-03-19-27-789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting batch transform job...\n",
      "  Input: s3://sagemaker-us-east-1-115718999037/mlops-pipeline/processed/batch\n",
      "  Output: s3://sagemaker-us-east-1-115718999037/mlops-pipeline/batch-output\n",
      "..............................\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:32:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:32:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:32:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:24:32 +0000] [18] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:24:32 +0000] [18] [INFO] Listening at: unix:/tmp/gunicorn.sock (18)\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:24:32 +0000] [18] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:24:32 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:24:32 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:24:32 +0000] [25] [INFO] Booting worker with pid: 25\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:24:32 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:37:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Oct/2025:03:24:37 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Oct/2025:03:24:37 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m2025-10-17T03:24:37.616:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Oct/2025:03:24:39 +0000] \"POST /invocations HTTP/1.1\" 200 1265027 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:39:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Oct/2025:03:24:39 +0000] \"POST /invocations HTTP/1.1\" 200 1265027 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:39:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Oct/2025:03:24:39 +0000] \"POST /invocations HTTP/1.1\" 200 567037 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Oct/2025:03:24:39 +0000] \"POST /invocations HTTP/1.1\" 200 1264203 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Oct/2025:03:24:39 +0000] \"POST /invocations HTTP/1.1\" 200 567037 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Oct/2025:03:24:39 +0000] \"POST /invocations HTTP/1.1\" 200 1264203 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\n",
      "\n",
      " Waiting for transform job to complete...\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:32:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:32:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:32:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:32:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:32:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:32:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:24:32 +0000] [18] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:24:32 +0000] [18] [INFO] Listening at: unix:/tmp/gunicorn.sock (18)\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:24:32 +0000] [18] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:24:32 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:24:32 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:24:32 +0000] [25] [INFO] Booting worker with pid: 25\u001b[0m\n",
      "\u001b[34m[2025-10-17 03:24:32 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2025-10-17 03:24:32 +0000] [18] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[35m[2025-10-17 03:24:32 +0000] [18] [INFO] Listening at: unix:/tmp/gunicorn.sock (18)\u001b[0m\n",
      "\u001b[35m[2025-10-17 03:24:32 +0000] [18] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2025-10-17 03:24:32 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[35m[2025-10-17 03:24:32 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[35m[2025-10-17 03:24:32 +0000] [25] [INFO] Booting worker with pid: 25\u001b[0m\n",
      "\u001b[35m[2025-10-17 03:24:32 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:34:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:34:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:34:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:34:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:34:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:34:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:34:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:34:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:34:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:34:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:34:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:34:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:34:INFO] Model objective : binary:logistic\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:37:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Oct/2025:03:24:37 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Oct/2025:03:24:37 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:37:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Oct/2025:03:24:37 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Oct/2025:03:24:37 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:38:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m2025-10-17T03:24:37.616:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Oct/2025:03:24:39 +0000] \"POST /invocations HTTP/1.1\" 200 1265027 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:39:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Oct/2025:03:24:39 +0000] \"POST /invocations HTTP/1.1\" 200 1265027 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:39:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-10-17:03:24:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Oct/2025:03:24:39 +0000] \"POST /invocations HTTP/1.1\" 200 567037 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Oct/2025:03:24:39 +0000] \"POST /invocations HTTP/1.1\" 200 1264203 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2025-10-17:03:24:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Oct/2025:03:24:39 +0000] \"POST /invocations HTTP/1.1\" 200 567037 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Oct/2025:03:24:39 +0000] \"POST /invocations HTTP/1.1\" 200 1264203 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\n",
      " Batch transform completed successfully!\n",
      "\n",
      " Batch transform complete!\n",
      "   Output location: s3://sagemaker-us-east-1-115718999037/mlops-pipeline/batch-output\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE: Deploy batch transform\n",
    "batch_transformer = deploy_batch_transform(trained_estimator, training_job_name)\n",
    "\n",
    "print(\"\\n Batch transform complete!\")\n",
    "print(f\"   Output location: {batch_transformer.output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08285bb6-61fb-4fd8-a598-04f80b5ffc32",
   "metadata": {},
   "source": [
    "## Section 8: Model and Data Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03719a88-1043-452f-9422-737cd320e386",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T03:25:44.135338Z",
     "iopub.status.busy": "2025-10-17T03:25:44.135034Z",
     "iopub.status.idle": "2025-10-17T03:25:44.142391Z",
     "shell.execute_reply": "2025-10-17T03:25:44.141733Z",
     "shell.execute_reply.started": "2025-10-17T03:25:44.135311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up model monitoring for deployed endpoint\n",
    "def setup_model_monitoring(endpoint_name):\n",
    "    \n",
    "    # Create data capture configuration\n",
    "    data_capture_config = DataCaptureConfig(\n",
    "        enable_capture=True,\n",
    "        sampling_percentage=100,\n",
    "        destination_s3_uri=f\"s3://{default_bucket}/{S3_PREFIX}/data-capture\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n Data Capture Configuration:\")\n",
    "    print(f\"  Sampling: 100%\")\n",
    "    print(f\"  Destination: s3://{default_bucket}/{S3_PREFIX}/data-capture\")\n",
    "    \n",
    "    # Create model monitor\n",
    "    monitor = DefaultModelMonitor(\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m5.xlarge',\n",
    "        volume_size_in_gb=20,\n",
    "        max_runtime_in_seconds=3600,\n",
    "    )\n",
    "    \n",
    "    # Suggest baseline\n",
    "    baseline_results_uri = f\"s3://{default_bucket}/{S3_PREFIX}/baselining\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n Suggesting baseline...\")\n",
    "        monitor.suggest_baseline(\n",
    "            baseline_dataset=f\"s3://{default_bucket}/{S3_PREFIX}/processed/train/train_commerce.csv\",\n",
    "            dataset_format={\"csv\": {\"header\": False}},\n",
    "            output_s3_uri=baseline_results_uri,\n",
    "            wait=True\n",
    "        )\n",
    "        print(f\" Baseline created: {baseline_results_uri}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: {str(e)}\")\n",
    "    \n",
    "    # Create monitoring schedule\n",
    "    schedule_name = f\"ecommerce-monitor-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n Creating monitoring schedule: {schedule_name}\")\n",
    "        monitor.create_monitoring_schedule(\n",
    "            monitor_schedule_name=schedule_name,\n",
    "            endpoint_input=endpoint_name,\n",
    "            output_s3_uri=f\"s3://{default_bucket}/{S3_PREFIX}/monitoring-output\",\n",
    "            statistics=monitor.baseline_statistics(),\n",
    "            constraints=monitor.suggested_constraints(),\n",
    "            schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "        )\n",
    "        print(f\" Monitoring schedule created\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: {str(e)}\")\n",
    "    \n",
    "    return monitor, schedule_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0c7e6844-a888-4c77-a5f4-133252f94785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T03:25:46.473082Z",
     "iopub.status.busy": "2025-10-17T03:25:46.472778Z",
     "iopub.status.idle": "2025-10-17T03:25:46.480689Z",
     "shell.execute_reply": "2025-10-17T03:25:46.480005Z",
     "shell.execute_reply.started": "2025-10-17T03:25:46.473057Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create CloudWatch dashboard for monitoring\n",
    "def create_cloudwatch_dashboard(endpoint_name):\n",
    "    \n",
    "    dashboard_name = f\"ECommerce-ML-Dashboard-{strftime('%Y%m%d', gmtime())}\"\n",
    "    \n",
    "    dashboard_body = {\n",
    "        \"widgets\": [\n",
    "            {\n",
    "                \"type\": \"metric\",\n",
    "                \"properties\": {\n",
    "                    \"metrics\": [\n",
    "                        [\"AWS/SageMaker\", \"ModelLatency\", {\"stat\": \"Average\"}],\n",
    "                        [\".\", \".\", {\"stat\": \"Maximum\"}]\n",
    "                    ],\n",
    "                    \"period\": 300,\n",
    "                    \"stat\": \"Average\",\n",
    "                    \"region\": region,\n",
    "                    \"title\": \"Model Latency\",\n",
    "                    \"yAxis\": {\"left\": {\"label\": \"Milliseconds\"}}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"metric\",\n",
    "                \"properties\": {\n",
    "                    \"metrics\": [\n",
    "                        [\"AWS/SageMaker\", \"Invocations\", {\"stat\": \"Sum\"}],\n",
    "                        [\".\", \"InvocationErrors\", {\"stat\": \"Sum\"}]\n",
    "                    ],\n",
    "                    \"period\": 300,\n",
    "                    \"stat\": \"Sum\",\n",
    "                    \"region\": region,\n",
    "                    \"title\": \"Invocations and Errors\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"metric\",\n",
    "                \"properties\": {\n",
    "                    \"metrics\": [\n",
    "                        [\"AWS/SageMaker\", \"CPUUtilization\", {\"stat\": \"Average\"}],\n",
    "                        [\".\", \"MemoryUtilization\", {\"stat\": \"Average\"}]\n",
    "                    ],\n",
    "                    \"period\": 300,\n",
    "                    \"stat\": \"Average\",\n",
    "                    \"region\": region,\n",
    "                    \"title\": \"Infrastructure Metrics\",\n",
    "                    \"yAxis\": {\"left\": {\"label\": \"Percent\"}}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"metric\",\n",
    "                \"properties\": {\n",
    "                    \"metrics\": [\n",
    "                        [\"AWS/SageMaker\", \"ModelSetupTime\", {\"stat\": \"Average\"}]\n",
    "                    ],\n",
    "                    \"period\": 300,\n",
    "                    \"stat\": \"Average\",\n",
    "                    \"region\": region,\n",
    "                    \"title\": \"Model Setup Time\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = cw_client.put_dashboard(\n",
    "            DashboardName=dashboard_name,\n",
    "            DashboardBody=json.dumps(dashboard_body)\n",
    "        )\n",
    "        print(f\"\\n✓ CloudWatch Dashboard created: {dashboard_name}\")\n",
    "        print(f\"  View at: https://console.aws.amazon.com/cloudwatch/home?region={region}#dashboards:name={dashboard_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: {str(e)}\")\n",
    "    \n",
    "    return dashboard_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "230d420c-9e6a-43ef-a5ca-1625ed4f2726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T03:25:48.039873Z",
     "iopub.status.busy": "2025-10-17T03:25:48.039599Z",
     "iopub.status.idle": "2025-10-17T03:25:48.045564Z",
     "shell.execute_reply": "2025-10-17T03:25:48.044845Z",
     "shell.execute_reply.started": "2025-10-17T03:25:48.039853Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create CloudWatch alarms for monitoring\n",
    "def create_cloudwatch_alarms():\n",
    "    \n",
    "    alarms = [\n",
    "        {\n",
    "            \"AlarmName\": f\"ECommerce-HighLatency-{strftime('%Y%m%d', gmtime())}\",\n",
    "            \"MetricName\": \"ModelLatency\",\n",
    "            \"Threshold\": 1000,\n",
    "            \"ComparisonOperator\": \"GreaterThanThreshold\",\n",
    "            \"Description\": \"Alert when model latency exceeds 1000ms\"\n",
    "        },\n",
    "        {\n",
    "            \"AlarmName\": f\"ECommerce-HighErrorRate-{strftime('%Y%m%d', gmtime())}\",\n",
    "            \"MetricName\": \"InvocationErrors\",\n",
    "            \"Threshold\": 10,\n",
    "            \"ComparisonOperator\": \"GreaterThanThreshold\",\n",
    "            \"Description\": \"Alert when invocation errors exceed 10\"\n",
    "        },\n",
    "        {\n",
    "            \"AlarmName\": f\"ECommerce-HighCPU-{strftime('%Y%m%d', gmtime())}\",\n",
    "            \"MetricName\": \"CPUUtilization\",\n",
    "            \"Threshold\": 80,\n",
    "            \"ComparisonOperator\": \"GreaterThanThreshold\",\n",
    "            \"Description\": \"Alert when CPU utilization exceeds 80%\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    created_alarms = []\n",
    "    for alarm in alarms:\n",
    "        try:\n",
    "            cw_client.put_metric_alarm(\n",
    "                AlarmName=alarm[\"AlarmName\"],\n",
    "                MetricName=alarm[\"MetricName\"],\n",
    "                Namespace=\"AWS/SageMaker\",\n",
    "                Statistic=\"Average\",\n",
    "                Period=300,\n",
    "                EvaluationPeriods=2,\n",
    "                Threshold=alarm[\"Threshold\"],\n",
    "                ComparisonOperator=alarm[\"ComparisonOperator\"],\n",
    "                AlarmDescription=alarm[\"Description\"]\n",
    "            )\n",
    "            print(f\" Created alarm: {alarm['AlarmName']}\")\n",
    "            created_alarms.append(alarm[\"AlarmName\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Note: {str(e)}\")\n",
    "    \n",
    "    return created_alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e322812-397b-4ad7-8ec3-4d239760627e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T03:27:02.455239Z",
     "iopub.status.busy": "2025-10-17T03:27:02.454967Z",
     "iopub.status.idle": "2025-10-17T03:27:02.871843Z",
     "shell.execute_reply": "2025-10-17T03:27:02.871227Z",
     "shell.execute_reply.started": "2025-10-17T03:27:02.455218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ CloudWatch Dashboard created: ECommerce-ML-Dashboard-20251017\n",
      "  View at: https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#dashboards:name=ECommerce-ML-Dashboard-20251017\n",
      " Created alarm: ECommerce-HighLatency-20251017\n",
      " Created alarm: ECommerce-HighErrorRate-20251017\n",
      " Created alarm: ECommerce-HighCPU-20251017\n",
      "\n",
      "Monitoring setup complete!\n",
      "   Dashboard: ECommerce-ML-Dashboard-20251017\n",
      "   Alarms created: 3\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE: Setup CloudWatch monitoring\n",
    "dashboard_name = create_cloudwatch_dashboard(\"ecommerce-endpoint\")\n",
    "alarm_names = create_cloudwatch_alarms()\n",
    "\n",
    "print(\"\\nMonitoring setup complete!\")\n",
    "print(f\"   Dashboard: {dashboard_name}\")\n",
    "print(f\"   Alarms created: {len(alarm_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ccf943-1c88-4bce-b1ba-fa1b1bb849c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:25:14.203588Z",
     "iopub.status.busy": "2025-10-17T02:25:14.203282Z",
     "iopub.status.idle": "2025-10-17T02:25:14.206861Z",
     "shell.execute_reply": "2025-10-17T02:25:14.206079Z",
     "shell.execute_reply.started": "2025-10-17T02:25:14.203562Z"
    }
   },
   "source": [
    "## Section 9: Model Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0a19899f-0cac-41c8-b20b-903474c879cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T03:27:47.273237Z",
     "iopub.status.busy": "2025-10-17T03:27:47.272967Z",
     "iopub.status.idle": "2025-10-17T03:27:47.278617Z",
     "shell.execute_reply": "2025-10-17T03:27:47.277881Z",
     "shell.execute_reply.started": "2025-10-17T03:27:47.273217Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "def create_evaluation_script():\n",
    "    \"\"\"Create evaluation script for model performance assessment.\"\"\"\n",
    "    \n",
    "    # Ensure the code directory exists\n",
    "    import os\n",
    "    os.makedirs('code', exist_ok=True)\n",
    "    print(\"→ Created/verified 'code' directory\")\n",
    "    \n",
    "    evaluation_script = \"\"\"import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load model\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "    \n",
    "    model = xgb.Booster()\n",
    "    model.load_model(\"xgboost-model\")\n",
    "    \n",
    "    # Load test data\n",
    "    test_path = \"/opt/ml/processing/test/test_commerce.csv\"\n",
    "    df_test = pd.read_csv(test_path, header=None)\n",
    "    \n",
    "    # Separate features and target\n",
    "    y_test = df_test.iloc[:, 0].values\n",
    "    X_test = df_test.iloc[:, 1:].values\n",
    "    \n",
    "    # Make predictions\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    predictions_prob = model.predict(dtest)\n",
    "    predictions = (predictions_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, zero_division=0)\n",
    "    recall = recall_score(y_test, predictions, zero_division=0)\n",
    "    f1 = f1_score(y_test, predictions, zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, predictions_prob)\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Create evaluation report\n",
    "    report = {\n",
    "        \"binary_classification_metrics\": {\n",
    "            \"accuracy\": {\"value\": float(accuracy)},\n",
    "            \"precision\": {\"value\": float(precision)},\n",
    "            \"recall\": {\"value\": float(recall)},\n",
    "            \"f1_score\": {\"value\": float(f1)},\n",
    "            \"auc\": {\"value\": float(auc)},\n",
    "            \"confusion_matrix\": {\n",
    "                \"true_negatives\": int(tn),\n",
    "                \"false_positives\": int(fp),\n",
    "                \"false_negatives\": int(fn),\n",
    "                \"true_positives\": int(tp)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save evaluation report\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    print(\"Evaluation complete!\")\n",
    "    print(json.dumps(report, indent=4))\n",
    "\"\"\"\n",
    "    \n",
    "    # Save the script\n",
    "    script_path = 'code/evaluation.py'\n",
    "    with open(script_path, 'w') as f:\n",
    "        f.write(evaluation_script)\n",
    "    \n",
    "    # Verify it was created\n",
    "    if os.path.exists(script_path):\n",
    "        file_size = os.path.getsize(script_path)\n",
    "        print(f\"✓ Evaluation script created: {script_path} ({file_size} bytes)\")\n",
    "    else:\n",
    "        print(f\"❌ ERROR: Failed to create {script_path}\")\n",
    "        raise IOError(f\"Could not create {script_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d596abc4-b10d-4dc3-83df-9130f65fcfbc",
   "metadata": {},
   "source": [
    "## Section 10: CI/CD Pipeline with SageMaker Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "17a0c1f7-5d5b-46b6-9adf-7d29b803b6a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T03:34:35.223918Z",
     "iopub.status.busy": "2025-10-17T03:34:35.223517Z",
     "iopub.status.idle": "2025-10-17T03:34:35.245817Z",
     "shell.execute_reply": "2025-10-17T03:34:35.245015Z",
     "shell.execute_reply.started": "2025-10-17T03:34:35.223885Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create complete CI/CD pipeline using SageMaker Pipelines\n",
    "def create_cicd_pipeline():\n",
    "    \n",
    "    # Define pipeline parameters\n",
    "    processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "    instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "    model_approval_status = ParameterString(\n",
    "        name=\"ModelApprovalStatus\",\n",
    "        default_value=\"PendingManualApproval\"\n",
    "    )\n",
    "    \n",
    "    # Upload data to S3 for pipeline\n",
    "    train_input_uri = f\"s3://{default_bucket}/{S3_PREFIX}/train_processed.csv\"\n",
    "    test_input_uri = f\"s3://{default_bucket}/{S3_PREFIX}/test_processed.csv\"\n",
    "    val_input_uri = f\"s3://{default_bucket}/{S3_PREFIX}/validation_processed.csv\"\n",
    "    batch_input_uri = f\"s3://{default_bucket}/{S3_PREFIX}/batch_commerce.csv\"\n",
    "    \n",
    "    input_data = ParameterString(name=\"InputDataTrain\", default_value=train_input_uri)\n",
    "    test_data = ParameterString(name=\"InputDataTest\", default_value=test_input_uri)\n",
    "    val_data = ParameterString(name=\"InputDataVal\", default_value=val_input_uri)\n",
    "    batch_data = ParameterString(name=\"BatchData\", default_value=batch_input_uri)\n",
    "    auc_threshold = ParameterFloat(name=\"AUCThreshold\", default_value=0.75)\n",
    "    \n",
    "    print(\"\\n Setting up pipeline steps...\")\n",
    "    \n",
    "    # Step 1: Processing\n",
    "    sklearn_processor = SKLearnProcessor(\n",
    "        framework_version=\"1.2-1\",\n",
    "        role=role,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        instance_count=1,\n",
    "        base_job_name=\"pipeline-preprocessing\",\n",
    "        sagemaker_session=pipeline_session\n",
    "    )\n",
    "    \n",
    "    processor_args = sklearn_processor.run(\n",
    "        code=\"code/preprocess.py\",\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=input_data,\n",
    "                destination=\"/opt/ml/processing/input/train\",\n",
    "                s3_data_distribution_type=\"FullyReplicated\"\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=test_data,\n",
    "                destination=\"/opt/ml/processing/input/test\",\n",
    "                s3_data_distribution_type=\"FullyReplicated\"\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=val_data,\n",
    "                destination=\"/opt/ml/processing/input/validation\",\n",
    "                s3_data_distribution_type=\"FullyReplicated\"\n",
    "            )\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train\",\n",
    "                source=\"/opt/ml/processing/output/train\",\n",
    "                destination=f\"s3://{default_bucket}/{S3_PREFIX}/pipeline/train\"\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test\",\n",
    "                source=\"/opt/ml/processing/output/test\",\n",
    "                destination=f\"s3://{default_bucket}/{S3_PREFIX}/pipeline/test\"\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"validation\",\n",
    "                source=\"/opt/ml/processing/output/validation\",\n",
    "                destination=f\"s3://{default_bucket}/{S3_PREFIX}/pipeline/validation\"\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"batch\",\n",
    "                source=\"/opt/ml/processing/output/batch\",\n",
    "                destination=f\"s3://{default_bucket}/{S3_PREFIX}/pipeline/batch\"\n",
    "            )\n",
    "        ],\n",
    "        arguments=[\n",
    "            \"--train-input\", \"train_processed.csv\",\n",
    "            \"--test-input\", \"test_processed.csv\",\n",
    "            \"--validation-input\", \"validation_processed.csv\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    step_process = ProcessingStep(name=\"PreprocessData\", step_args=processor_args)\n",
    "    print(\"  Processing step configured\")\n",
    "    \n",
    "    # Step 2: Training\n",
    "    image = sagemaker.image_uris.retrieve(\n",
    "        framework=\"xgboost\",\n",
    "        region=region,\n",
    "        version=\"1.7-1\"\n",
    "    )\n",
    "    \n",
    "    estimator = sagemaker.estimator.Estimator(\n",
    "        image,\n",
    "        role,\n",
    "        instance_count=1,\n",
    "        instance_type=instance_type,\n",
    "        volume_size=50,\n",
    "        input_mode=\"File\",\n",
    "        output_path=f\"s3://{default_bucket}/{S3_PREFIX}/pipeline/output\",\n",
    "        sagemaker_session=pipeline_session\n",
    "    )\n",
    "    \n",
    "    estimator.set_hyperparameters(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\",\n",
    "        max_depth=5,\n",
    "        eta=0.2,\n",
    "        gamma=4,\n",
    "        min_child_weight=6,\n",
    "        subsample=0.8,\n",
    "        verbosity=0,\n",
    "        num_round=100,\n",
    "    )\n",
    "    \n",
    "    train_args = estimator.fit(\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\"\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    step_train = TrainingStep(name=\"TrainModel\", step_args=train_args)\n",
    "    print(\"  Training step configured\")\n",
    "    \n",
    "    # Step 3: Evaluation\n",
    "    script_eval = ScriptProcessor(\n",
    "        image_uri=image,\n",
    "        command=[\"python3\"],\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        instance_count=1,\n",
    "        base_job_name=\"pipeline-evaluation\",\n",
    "        role=role,\n",
    "        sagemaker_session=pipeline_session,\n",
    "    )\n",
    "    \n",
    "    eval_args = script_eval.run(\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                destination=\"/opt/ml/processing/model\",\n",
    "            ),\n",
    "            ProcessingInput(\n",
    "                source=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/test\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"evaluation\",\n",
    "                source=\"/opt/ml/processing/evaluation\"\n",
    "            ),\n",
    "        ],\n",
    "        code=\"code/evaluation.py\",\n",
    "    )\n",
    "    \n",
    "    evaluation_report = PropertyFile(\n",
    "        name=\"EvaluationReport\",\n",
    "        output_name=\"evaluation\",\n",
    "        path=\"evaluation.json\"\n",
    "    )\n",
    "    \n",
    "    step_eval = ProcessingStep(\n",
    "        name=\"EvaluateModel\",\n",
    "        step_args=eval_args,\n",
    "        property_files=[evaluation_report],\n",
    "    )\n",
    "    print(\"  Evaluation step configured\")\n",
    "    \n",
    "    # Step 4: Create Model\n",
    "    model = sagemaker.model.Model(\n",
    "        image_uri=image,\n",
    "        model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "        sagemaker_session=pipeline_session,\n",
    "        role=role,\n",
    "    )\n",
    "    \n",
    "    step_create_model = ModelStep(\n",
    "        name=\"CreateModel\",\n",
    "        step_args=model.create(instance_type=\"ml.m5.large\"),\n",
    "    )\n",
    "    print(\"  Model creation step configured\")\n",
    "    \n",
    "    # Step 5: Batch Transform\n",
    "    transformer = estimator.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.m5.xlarge\",\n",
    "        output_path=f\"s3://{default_bucket}/{S3_PREFIX}/pipeline/batch-output\"\n",
    "    )\n",
    "    \n",
    "    step_transform = TransformStep(\n",
    "        name=\"BatchTransform\",\n",
    "        transformer=transformer,\n",
    "        inputs=TransformInput(\n",
    "            data=step_process.properties.ProcessingOutputConfig.Outputs[\"batch\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "    )\n",
    "    print(\"  Transform step configured\")\n",
    "    \n",
    "    # Step 6: Register Model\n",
    "    model_metrics = ModelMetrics(\n",
    "        model_statistics=MetricsSource(\n",
    "            s3_uri=\"{}/evaluation.json\".format(\n",
    "                step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "            ),\n",
    "            content_type=\"application/json\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    register_args = model.register(\n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"text/csv\"],\n",
    "        inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "        transform_instances=[\"ml.m5.xlarge\"],\n",
    "        model_package_group_name=MODEL_PACKAGE_GROUP_NAME,\n",
    "        approval_status=model_approval_status,\n",
    "        model_metrics=model_metrics,\n",
    "    )\n",
    "    \n",
    "    step_register = ModelStep(name=\"RegisterModel\", step_args=register_args)\n",
    "    print(\"  Registration step configured\")\n",
    "    \n",
    "    # Step 7: Conditional Deployment\n",
    "    step_fail = FailStep(\n",
    "        name=\"ModelPerformanceFail\",\n",
    "        error_message=Join(\n",
    "            on=\" \",\n",
    "            values=[\"Model AUC below threshold:\", auc_threshold]\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    cond_gte = ConditionLessThanOrEqualTo(\n",
    "        left=JsonGet(\n",
    "            step_name=step_eval.name,\n",
    "            property_file=evaluation_report,\n",
    "            json_path=\"binary_classification_metrics.auc.value\",\n",
    "        ),\n",
    "        right=auc_threshold,\n",
    "    )\n",
    "    \n",
    "    step_cond = ConditionStep(\n",
    "        name=\"CheckModelPerformance\",\n",
    "        conditions=[cond_gte],\n",
    "        if_steps=[step_register, step_create_model, step_transform],\n",
    "        else_steps=[step_fail],\n",
    "    )\n",
    "    print(\"  Conditional step configured\")\n",
    "    \n",
    "    # Create Pipeline\n",
    "    pipeline_name = f\"ECommercePipeline-{strftime('%Y%m%d-%H%M%S', gmtime())}\"\n",
    "    pipeline = Pipeline(\n",
    "        name=pipeline_name,\n",
    "        parameters=[\n",
    "            processing_instance_count,\n",
    "            instance_type,\n",
    "            model_approval_status,\n",
    "            input_data,\n",
    "            test_data,\n",
    "            val_data,\n",
    "            batch_data,\n",
    "            auc_threshold,\n",
    "        ],\n",
    "        steps=[step_process, step_train, step_eval, step_cond],\n",
    "        sagemaker_session=pipeline_session,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n Creating pipeline: {pipeline_name}\")\n",
    "    pipeline.upsert(role_arn=role)\n",
    "    print(f\" Pipeline created successfully!\")\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e5c24b33-fc85-4fed-930f-5095dee8ff4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T03:34:37.229907Z",
     "iopub.status.busy": "2025-10-17T03:34:37.229628Z",
     "iopub.status.idle": "2025-10-17T03:34:37.233667Z",
     "shell.execute_reply": "2025-10-17T03:34:37.232970Z",
     "shell.execute_reply.started": "2025-10-17T03:34:37.229887Z"
    }
   },
   "outputs": [],
   "source": [
    "# Execute the CI/CD pipeline\n",
    "def execute_pipeline(pipeline):\n",
    "    \n",
    "    print(f\"\\n Starting pipeline execution...\")\n",
    "    execution = pipeline.start()\n",
    "    \n",
    "    print(f\" Pipeline execution started\")\n",
    "    print(f\"  Execution ARN: {execution.arn}\")\n",
    "    \n",
    "    return execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3127358-f9b9-4cd5-a000-9bc01b7324b4",
   "metadata": {},
   "source": [
    "## Section 11: Model Registry and Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf08ac5e-74ed-4951-9892-2ffd64aa1ea2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T02:28:04.463573Z",
     "iopub.status.busy": "2025-10-17T02:28:04.463289Z",
     "iopub.status.idle": "2025-10-17T02:28:04.467798Z",
     "shell.execute_reply": "2025-10-17T02:28:04.467167Z",
     "shell.execute_reply.started": "2025-10-17T02:28:04.463551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up Model Registry for model versioning\n",
    "def setup_model_registry():\n",
    "    \n",
    "    model_package_group_description = (\n",
    "        'Model package group for E-Commerce Recommendation System. '\n",
    "        'Tracks all trained model versions with performance metrics.'\n",
    "    )\n",
    "    \n",
    "    model_package_group_input = {\n",
    "        \"ModelPackageGroupName\": MODEL_PACKAGE_GROUP_NAME,\n",
    "        \"ModelPackageGroupDescription\": model_package_group_description,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = sm_client.create_model_package_group(**model_package_group_input)\n",
    "        print(f\"\\n✓ Model Package Group created: {MODEL_PACKAGE_GROUP_NAME}\")\n",
    "        print(f\"  ARN: {response['ModelPackageGroupArn']}\")\n",
    "    except Exception as e:\n",
    "        if \"already exists\" in str(e):\n",
    "            print(f\"\\n✓ Model Package Group already exists: {MODEL_PACKAGE_GROUP_NAME}\")\n",
    "        else:\n",
    "            print(f\"Note: {str(e)}\")\n",
    "    \n",
    "    return MODEL_PACKAGE_GROUP_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807266e-4aeb-4a88-ad10-e0ebf6dd36b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
